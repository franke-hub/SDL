##############################################################################
##
##       Copyright (C) 2022-2023 Frank Eskesen.
##
##       This file is free content, distributed under the MIT license.
##       (See accompanying file LICENSE.MIT or the original contained
##       within https://opensource.org/licenses/MIT)
##
##############################################################################
##
## Title-
##       .README
##
## Purpose-
##       README: List of usage notes, bugs, fixes, etc.
##
## Last change date-
##       2023/05/28
##
##############################################################################

##############################################################################
## STATUS:
2023/05/28 FIXED: Issue #3
         BUG: T_Stream --major=1 unstable running heavy stress testing
         Client and Server inp_task close procesing deadlocks because
         inp_task invokes close which removes the last shared_ptr<Server>
         reference. Even adding a keep_alive shared_ptr in task_inp doesn't
         help enough because the Client/Server destructor is invoked under
         task_inp control *before* dispatcher processing for the close
         operation completes.
         This results in a deadlock since in dispatch::Task::~Task a wait
         operation is enqueued and waited for while task_inp is running.

         FIX: In inp_task, we transfer posting of the CLOSE work item onto
         an internal Dispatcher task created just for this purpose. When that
         Task posts (and deletes) the CLOSE work item, the last Client/Server
         reference is removed and the Task destructor runs under that task,
         removing the deadlock.

         We similarly also defer posting out_task work items. This is needed
         on Linux, but it's not clear why. (We know the deadlock occurs, but
         don't know the exact interaction that causes it.)

##############################################################################
## SOCKET INFO: (Coding reference information)
typedef __uint32_t     in_addr_t;   // netinet/in.h => cygwin/in.h
typedef __uint16_t     in_port_t;   // netinet/in.h => cygwin/in.h

typedef __sa_family_t  sa_family_t; // sys/socket.h
typedef __socklen_t    socklen_t;   // sys/socket.h

#define AF_*; #define PF_*;         // cygwin/socket.h

struct addrinfo {                   // netdb.h
int                    ai_flags;    // input flags
int                    ai_family;   // address family of socket
int                    ai_socktype; // socket type
int                    ai_protocol; // ai_protocol
socklen_t              ai_addrlen;  // length of socket address
char*                  ai_canonname; // canonical name of service location
struct sockaddr*       ai_addr;     // socket address of socket
struct addrinfo*       ai_next;     // pointer to next in list
};

struct hostent {                    // netdb.h
char*                  h_name;      // Official name of host
char**                 h_aliases;   // Alias list
int                    h_addrtype;  // Host address type
int                    h_length;    // Length of address
char**                 h_addr_list; // List of addresses
};

struct in_addr {                    // netinet/in.h => cygwin/in.h
in_addr_t              s_addr;      // Internet address
};

struct in6_addr {                   // netinet/in.h
union {
   int8_t              __s6_addr[16];
   int16_t             __s6_addr16[8];
   int32_t             __s6_addr32[4];
} __u6;
#define s6_addr   __u6.__s6_addr
#define s6_addr16 __u6.__s6_addr16
#define s6_addr32 __u6.__s6_addr32
};

struct linger {                     // sys/socket.h
unsigned short         l_onoff;     // Linger active?
unsigned short         l_linger;    // Linger timeout, in seconds
};

struct pollfd {                     // sys/poll.h
  int fd;
  short events;
  short revents;
};

struct sockaddr {                   // sys/socket.h
sa_family_t            sa_family;   // Address family
char                   sa_data[14];
};

struct sockaddr_in {                // netinet/in.h => cygwin/in.h
sa_family_t            sin_family;  // AF_INET
in_port_t              sin_port;    // Port number
struct in_addr         sin_addr;    // Internet address
char                   __pad[*];
};

struct sockaddr_in6 {               // netinet/in.h => * => cygwin/in6.h
sa_family_t            sin6_family; // AF_INET6
in_port_t              sin6_port;   // Port number
uint32_t               sin6_flowinfo; // Traffic class and flow info
struct in6_addr        sin6_addr;   // IPv6 address
uint32_t               sin6_scope_id; // Set of interfaces for a scope
};

struct sockaddr_storage {           // sys/socket.h
sa_family_t            ss_family;   // Address family
char                   __ss_pad1[*];
int64_t                __ss_align;
char                   __ss_pad2[*];
};

union sockaddr_x {                  // pub/Socket.h (pub::Socket::sockaddr_x)
sa_family_t            x_family;    // Socket address family
uint16_t               _0002[10];   // (Unused)
uint16_t               x_socksize;  // Length(x_sockaddr)
sockaddr*              x_sockaddr;  // Sockaddr copy (possibly this)
};

union sockaddr_u {                  // pub/Socket.h (pub::Socket::sockaddr_u)
uint64_t               su_align[4]; // Alignment and maximum size (32)
sa_family_t            su_af;       // Socket address family
sockaddr               sa;          // Socket address
sockaddr_in            su_i4;       // IPv4 internet address
sockaddr_in6           su_i6;       // IPv6 internet address
sockaddr_x             su_x;        // Extended sockaddr format

std::string to_string( void ) const; // Convert to display string
}; // union sockaddr_u

##############################################################################
## STATUS: (History)
2022/04/12 HTTP1 Stable, improved throughput
         Unable to reproduce Cygwin performance anomaly.

2022/03/06 HTTP1 Stable, improved throughput

2022/12/18 HTTP1 Stable
         No known multithreading timing holes exist.
2022/12/10 Asychronous Client/Server I/O working well.
         Timing exposures still exist. Some redesign is needed.
2022/10/23 Asychronous Client/Server I/O working, but with known bugs.
2022/10/15 Asychronous Client I/O working. Server Client I/O is synchronous.
2022/09/25 Asychronous Client I/O working (with one client)
2022/08/31 PROTOCOL1A (since renamed to HTTP1) working with updated Socket.
2022/07/22 PROTOCOL1B (since renamed to PROTOCOL1A) working.
2022/07/16 Moved source into distribution library.
2022/06/18 Resumed active development
         ~/src/cpp/lib/pub/Test/TestSock.cpp working
2021/11/01 Deferred: building ~/src/cpp/lib/pub/Test/TestSock.cpp to learn
         more about SSL sockets and sockets in general.
2021/11/01 Started

##############################################################################
## BUGS: (Open bugs)
2023/04/24 T_Stream --major=1 unstable running heavy stress testing

##############################################################################
## FIXED: (Fixed bugs)
2023/05/18 Client deleted while active.
         Added shared_ptr<Client> to ClientItem.
         --major stress runs longer but still doesn't complete sucessfully.

2022/12/12 Performance issue
         Throughput is pathetic running T_Stream --server --stress --major=1.
         (The --major=1 option disallows connection reuse, i.e.
         each request requires a new connection.)
2023/04/24 (Partial fix: performance improved, but stress testing iffy.)

2022/12/18 Server.cpp, Client.cpp, Listen.cpp: Possible timing issue
         Fixes were also required in ../pub/Select.cpp, ../pub/Socket.cpp.
         Select.cpp now calls Socket::do_select with its shr_latch held.
         Client.cpp, Listen.cpp, and Server.cpp no longer call close()
         from the asynch() method. They offload that work to their input task.

2022/12/12 Linux stress test completes but doesn't terminate properly
         when more than ~10 client threads are used.

2022/12/12 Intermittent termination error: "609 S/Select.cpp Should not
         occur (but did)". The associated debug shows a closed socket
         handle still active in the pollfd and sarray tables.

2022/11/17 Ioda operator+=(Ioda&&) inserted at head rather than at tail
         TODO: Maybe make that operator-=(string&), since that might've been
         used instead in ServerResponse::write(). (Use all operator -='s)

2022/10/24 Linux throughput performance is pathetic.
         Adding more Server threading fixed the problem.

##############################################################################
## CLOSED: (Bugs closed without fixing)
2022/10/24 Cygwin performance issue:
         Cygwin stress test works best with --stress=1 (one client.)
         When more than one client is used, throughput decreases.
2023/04/12 (CANNOT REPRODUCE)

##############################################################################
## TODO: (Deferred)
2022/02/16 TODO? Don't wait for content-length
2022/02/16 TODO? Allow next HTTP1 write before response

##############################################################################
## DONE:
2022/12/18 Client connect operation moved from constructor.
         If IPV4 connect fails, we can retry with IPV6 connect.

2022/11/17 Need to stress test Select more.
         To test a Client/Server for each operation use:
           T_Stream --server --stress --major=1

2022/10/24 Rename ServerAgent to ListenAgent
         Server, ServerRequest/Response keep their current names

##############################################################################
## FEATURE: (Ideas for improvement)
2022/10/24 Support HTTP/2 (First w/o encryption)

